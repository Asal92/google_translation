'''
11/11/2022

Renamed some variables to make them more clear.
Looks like it's not actually adding sentences because it's expecting there to be a space rather than these underlines so I'm confused.

I don't know what the preprocess function is for.

When I try to run the code, it doesn't seem to work because no sentences are added.

I'm going to actually switch to use Ku's branch rather than this one.

Unsure if I should rebase or how to do this best.
I'm worried that a merge will keep some changes from Asal or me that I don't want.
If I create a new branch, I have to rename this one.

That should actually be fine.
That's what I'll do, then.

Great, on the new branch now!

It's looking likely that neither Ku nor Asal followed the paper.
The paper doesn't linearize the tags while doing the Google Translate conversion.
Should I follow what my group has already done and call it a new technique?
Or should I stick to the paper and do it the right way?

Let's cut down the number of sentences we translate and see how long it takes / what the format of the output is like.

Ooh we actually already have examples of the output. Great!

Let's do it for French and see what happens! I think the postprocessing code will be broken but that's okay.
'''

'''
11/12/2022

Here's what I posted in the chat last night:
    Finally got a handle on the code today. Should have the datasets we need (with translation of entities and without translation of entities) by tomorrow's meeting as promised.
    One thing that I found in the code is that we weren't following the MulDA method described in the paper. So, I'm going to rewrite it so it works the way MulDA does so that we're closer to the original. The main thing is that MulDA never linearizes the tags while doing the translation. As described in the document, MulDA has three steps - 1: translate with the entity tags (not linearization) 2: translate with the original term in brackets (not linearization) 3: replace the entity tags with the translated term (in our case, replace the entity tags with the non-translated term). The linearization (DAGA stuff) doesn't come into MulDA until the generation step, which we're not doing.
    Another thing is that we seem to be replacing unicode symbols by hand, which shouldn't be needed at all. As long as the data is read into our AI model using UTF-8 encoding, those \u and & values should remain in the .txt files. See these questions (1, 2, and 3) for more information.


So, what I need to do is essentially recode this from scratch.
The Google format is pretty easy to deal with, as is the MulDA stuff, so I'm not too worried.
Let's do it!

At some point we'll need a dictionary where an index points to the example number and the index of the word that's being translated in the sentence.

I think Ku was also missing some labels.

This sentence makes the bracket idea not work:
    siegfried ( ca . 1906 ) photo by aimé dupont ]]

And there's another sentence that has braces:
    inome _ _ O
    fishing _ _ O
    port _ _ O
    izumo _ _ B-HumanSettlement
    shimane _ _ B-ORG
    prefecture _ _ I-ORG
    japan _ _ B-HumanSettlement
    { _ _ O
    nwp _ _ O
    } _ _ O

So I think using quotes will be the best idea.

In order to get this to work in time, I'll be ignoring sentences without any entities.

We're going to be doing it slightly different than MulDA in that we're not going to be substituting the token in. We're just going to surround it in quotes.

I think ours should work better anyways.
Would be a good thing to test later.

I'm untranslating each entity individually.
Can write more code later to untranslate them all together.

Looks like something is off with the tabs

It also looks like something is wrong with the translated one (not the orig one)

This example is tricky:
    {
    "translatedText": "en collaboration avec \"robert gompf\", il a d\u00e9couvert des mod\u00e8les quadridimensionnels de la topologie de l'espace-temps.",
    "input": "jointly with \"robert gompf\" he discovered four dimensional models of space time topology ."
    },

It looks like in most examples the comma is its own token, unless it's in a number.

So I think we need to separate things off of quotes if it happens.

I've currently turned the quotes into backticks.

I think we will need to merge them all together, otherwise the AI will train incorrectly.

Okay, I may have made a huge mistake.

If it's translating each of the bracketed words separately, how do we know the default to put them in?
We can't put the translated ones back into the original one, because the original one isn't translated.
We also don't know where the non-bracketed ones went.

Actually, what if we take out the brackets? Shouldn't all of them be the same?
Yes. If they're not, we should ignore that example. That makes sense!

I think for the time being I'm going to skip the complicated cases with the symbols attached to the brackets.

Quotes in French are converted into "«»" also known as Guillemets, so I don't think we want to use quotes as the brackets.

I'm going to see if the same thing happens to backticks.

If so, then I'll just skip examples where that happens.

This site is super helpful for seeing what characters are in text:
    https://www.soscisurvey.de/tools/view-chars.php

Everything says its a tab, so I'm just going to go with it.

With quotes we end up skipping a lot:
    Skipped 8559/16778 sentences

Let's try backticks!

Even worse:
    Skipped 12808/16778 sentences
    Skipped 0 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 10975 sentences due to brackets not being found
    Skipped 1198 sentences due to translations not matching
    Skipped 635 sentences due to template token mismatch (usually from punctuation added after an entity)

Looks like it converts backticks into quotes at times too.

What if we actually use brackets?

It fails - I'm going to make it so it doesn't.

With square bracketing:
    Skipped 5858/16778 sentences
    Skipped 72 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 54 sentences due to brackets not being found
    Skipped 4153 sentences due to translations not matching
    Skipped 1579 sentences due to template token mismatch (usually from punctuation added after an entity)

With curly braces:
    Skipped 5780/16778 sentences
    Skipped 1 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 49 sentences due to brackets not being found
    Skipped 4087 sentences due to translations not matching
    Skipped 1643 sentences due to template token mismatch (usually from punctuation added after an entity)

It seems like a lot of the curly braces ones are because it's literally not translating the word in all cases.

With angular brackets:
    Skipped 5599/16778 sentences
    Skipped 5 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 98 sentences due to brackets not being found
    Skipped 4039 sentences due to translations not matching
    Skipped 1457 sentences due to template token mismatch (usually from punctuation added after an entity)

With parentheses:
    Skipped 7440/16778 sentences
    Skipped 2459 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)        
    Skipped 108 sentences due to brackets not being found
    Skipped 3526 sentences due to translations not matching
    Skipped 1347 sentences due to template token mismatch (usually from punctuation added after an entity)




TEMPLATE TOKEN MISMATCH ISSUE FIXED

I'm going to try quotes again now that I've fixed this.

Quotes (with template token mismatch issue fixed):
    Skipped 5304/16778 sentences
    Skipped 0 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 1823 sentences due to brackets not being found
    Skipped 3481 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

Next is curly braces.

Curly braces:
    Skipped 4129/16778 sentences
    Skipped 1 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 48 sentences due to brackets not being found
    Skipped 4080 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

We have to try their standard of square brackets.
Square brackets:
    Skipped 4273/16778 sentences
    Skipped 72 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 52 sentences due to brackets not being found
    Skipped 4149 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

Caret (^):
    Skipped 4973/16778 sentences
    Skipped 0 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 922 sentences due to brackets not being found
    Skipped 4049 sentences due to translations not matching
    Skipped 2 sentences due to template token mismatch (usually from punctuation added after an entity)

Looks like sometimes I get different results! But overall not too different.

Let's see what happens if I use ones with different lengths now.

The one rule is that neither bracket should be a strict substring of the other bracket.

Oops I didn't do that I still did the same length:
Double dashes (--):
    Skipped 5335/16778 sentences
    Skipped 1 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 1395 sentences due to brackets not being found
    Skipped 3939 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

I think I should just add space around punctuation naturally.

There is a string.punctuation, but I feel like using that would take a while to program.

Mismatched Asterisk Square Bracket (*[ ]):
    Skipped 5798/16778 sentences
    Skipped 71 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 2281 sentences due to brackets not being found
    Skipped 3446 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

Bars (||):
    Skipped 5672/16778 sentences
    Skipped 63 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 192 sentences due to brackets not being found
    Skipped 5417 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

I still have no idea how a token mismatch could happen with the carets.

Final run with square brackets:
    Skipped 4278/16778 sentences
    Skipped 72 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 54 sentences due to brackets not being found
    Skipped 4152 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)

Final run from fr to en (square brackets):
    Skipped 4111/16548 sentences
    Skipped 42 sentences due to invalid bracketing (usually from the original sentence containing the bracket characters)
    Skipped 77 sentences due to brackets not being found
    Skipped 3992 sentences due to translations not matching
    Skipped 0 sentences due to template token mismatch (usually from punctuation added after an entity)
'''

'''
11/15/2022

Okay here's a good example:
{
  "translatedText": "la soci\u00e9t\u00e9 a \u00e9t\u00e9 achet\u00e9e par [john waddington limited] en 1963 qui l'a vendue \u00e0 son tour \u00e0 des cartes de marque en 1980 .",
  "input": "the company was purchased by [john waddington limited] in 1963 who sold it in turn to hallmark cards in 1980 ."
 },
 {
  "translatedText": "la soci\u00e9t\u00e9 a \u00e9t\u00e9 achet\u00e9e par john waddington limited en 1963 qui l'a vendue \u00e0 son tour \u00e0 [hallmark cards] en 1980 .",
  "input": "the company was purchased by john waddington limited in 1963 who sold it in turn to [hallmark cards] in 1980 ."
 },

 Actualy I like the wrestling one more.

 {
  "translatedText": "ils ont fait \u00e9quipe de 1989 \u00e0 1991 dans la [national wrestling alliance] (nwa) et le championnat du monde de lutte (www).",
  "input": "they teamed from 1989 to 1991 in the [national wrestling alliance] ( nwa ) and world championship wrestling ( wcw ) ."
 },
 {
  "translatedText": "ils ont fait \u00e9quipe de 1989 \u00e0 1991 dans l'alliance nationale de lutte (nwa) et [la lutte du championnat du monde] (wcw).",
  "input": "they teamed from 1989 to 1991 in the national wrestling alliance ( nwa ) and [world championship wrestling] ( wcw ) ."
 },

Here's an example that's short that shows it working.
  {
  "translatedText": "il est exprim\u00e9 par [mitsuko horie] dans le premier anime et par motoko kumai dans le second.",
  "input": "he is voiced by [mitsuko horie] in the first anime and by motoko kumai in the second ."
 },
 {
  "translatedText": "il est exprim\u00e9 par mitsuko horie dans le premier anime et par [motoko kumai] dans le second.",
  "input": "he is voiced by mitsuko horie in the first anime and by [motoko kumai] in the second ."
 },



{
"translatedText": "[edward weston] chimiste et ing\u00e9nieur concurrent de thomas edison",
"input": "[edward weston] chemist and engineer competitor with thomas edison"
},
{
"translatedText": "edward weston chimiste et ing\u00e9nieur concurrent de [thomas edison]",
"input": "edward weston chemist and engineer competitor with [thomas edison]"
},

Okay, here's just a list of weirdly tagged examples: (put these on slide 14)
# id 156a76ec-4fe3-42ed-9bcc-ec550894cf08	domain=en
he _ _ O
holds _ _ O
wins _ _ O
over _ _ O
tito _ _ B-Athlete
ortiz _ _ I-Athlete
masakatsu _ _ B-Athlete
funaki _ _ I-Athlete
yuki _ _ B-OtherPER
kondo _ _ I-OtherPER
semmy _ _ B-Athlete
schilt _ _ I-Athlete
and _ _ O
minoru _ _ B-Athlete
suzuki _ _ I-Athlete
. _ _ O

# id 1de39462-f3b7-41b2-8605-5a8cf6e4e220	domain=en
ray _ _ B-Athlete
ferraro _ _ I-Athlete
( _ _ O
select _ _ O
games _ _ O
) _ _ O
jamie _ _ B-Athlete
mclennan _ _ I-Athlete
( _ _ O
select _ _ O
games _ _ O
) _ _ O
mike _ _ B-OtherPER
johnson _ _ I-OtherPER
( _ _ O
select _ _ O
games _ _ O
) _ _ O



Here's the example I'll actually use for the slide:
 {
  "translatedText": "la [r\u00e9action en cha\u00eene par polym\u00e9rase] a \u00e9t\u00e9 d\u00e9velopp\u00e9e dans les ann\u00e9es 1980 par kary mullis .",
  "input": "the [polymerase chain reaction] was developed in the 1980s by kary mullis ."
 },
 {
  "translatedText": "la r\u00e9action en cha\u00eene par polym\u00e9rase a \u00e9t\u00e9 d\u00e9velopp\u00e9e dans les ann\u00e9es 1980 par [kary mullis] .",
  "input": "the polymerase chain reaction was developed in the 1980s by [kary mullis] ."
 },
'''

'''
12/3/2022

Alright, it's time to actually implement MulDA in the next few hours.
It shouldn't be too hard.
Step 1: Replace entities with the names.
Step 2: Translate the sentence.
Step 3: Translate the sentence with brackets for each individual word.
Step 4: Replace the names with the original entities and the non-translated entities.

Step 3 can probably load from an existing file. I've already done all those translations.

Yup, Step 3 is definitely in `results.json` files.

So I should just be able to reload those.

So the goal of this script is to again output an orig and a trans.
Sweet.

Here's another bad training example:
    dimeglio _ _ O
    also _ _ O
    had _ _ O
    a _ _ O
    brief _ _ O
    stint _ _ O
    in _ _ O
    world _ _ B-PrivateCorp
    championship _ _ I-PrivateCorp
    wrestling _ _ I-PrivateCorp
    with _ _ O
    romeo _ _ B-Athlete
    valentino _ _ I-Athlete
    during _ _ O
    the _ _ O
    mid _ _ O
    1990s _ _ O
    . _ _ O

Alright, I think I've got the main portions solid.

I renamed the old outputs so it's clear that they're london outputs rather than MulDA outputs.
Will need to rename these outputs in the london_translate script.

So many issues:
{
  "translatedText": "OtherPER0 1939 Vainqueur de VisualWork1 et fondateur d'ORG2",
  "input": "OtherPER0 1939 VisualWork1 winner and founder of ORG2"
 },
 {
  "translatedText": "sous le r\u00e8gne de l'AutrePER0 ( r . 1861 \u2013 1875 ) :",
  "input": "during the reign of the OtherPER0 ( r . 1861 \u2013 1875 ) :"
 },

So the question is whether or not we should fix them or just run with it.
Lemme look at their code to see if they do anything special like putting "no translate" spans around it.

Wow. Their code is a mess.
I see why Asal tried to do all the fancy replacement stuff - they did too!
That seems like an issue.

There are also lines like:
    if tl == 'nl':
        t = t.replace("PERO","PER0")

Which just make me think that they really messed up and had a really hacky codebase.

Just used ChatGPT to try to understand some of it, and yeah, it's an absolute mess.

So to just follow MulDA would likely result in a lot of dropped examples.
Let's create a list of issues we see with French:
    1. "l'" "d'" and other things like this being added.
    2. Words like "OTHER" in "OTHERPER" or "POLITICIAN" being translated
        Note that this is not consistent, even among labels:
            {
            "translatedText": "sa fille Politician0 a jou\u00e9 le r\u00f4le de premi\u00e8re dame pendant sa pr\u00e9sidence.",
            "input": "her daughter Politician0 served in the role of first lady during her presidency ."
            },
            {
            "translatedText": "19 juin \u2013 Politicien0 membre du congr\u00e8s (mort de WrittenWork1)",
            "input": "june 19 \u2013 Politician0 congressman ( died WrittenWork1 )"
            }

Those seem to be the main issues found over and over again.
So let's do two versions, lol.
One where we deal with these issues, and one where we don't.

Okay, so we know that French does the "d'" and "l'" (and "s'") thing when the next word starts with a vowel.
However, the entities may start with a vowel when the term itself does not.

Also, the "de", "le", and "se" are important for the translation.

This seems like it's going to be really complex.
So, for now, I think I'm just going to do it the simple way and then ask the group how they'd like to handle it later today.

I was thinking that the function could just treat it as a single word, but then realized the liason would be all wrong for those examples where the entity doesn't start with a vowel.

Alright, after talking with team, we just decided to go the simple route.
If it seems to drop too many examples, we may try to fix problem #2 (where it translates them).

Wait. If the London translate skips over examples *before* the translation, then that will mess up the examples found.
Because the London translate will have a different number of entities / will have less sentences.

Well, then, maybe I just process the london results to make sure the sentences match.
I'll I've done is put brackets around words. So if I remove all the brackets, I should get the original sentence.

That's correct. The `check_brackets` function ensures that the brackets are the only ones in there.

I could rename things "stable" instead of "London", but I think that would just take extra time with no functionality.

The code's not finding anything from the London results, so I'll have to look into that.

Athlète is another one in addition to Politicien and AutrePER.

Wow, this dataset is a mess.
The current bug is because some of the examples in the dataset are included twice.
So when I go through the london translation, it shows up twice, and adds twice the examples.

Sometimes, the same example is translated differently.

 {
  "translatedText": "comme l'oiseau tout aussi grandeur nature du cygne menac\u00e9 de jan asselijn, le taureau peut \u00e9galement fonctionner comme un symbole de la [r\u00e9publique n\u00e9erlandaise].",
  "input": "like the equally life size bird in jan asselijn 's the threatened swan the bull can also function as a symbol of the [dutch republic] ."
 },
 {
  "translatedText": "comme l'oiseau tout aussi grandeur nature du cygne menac\u00e9 de jan asselijn, le taureau peut \u00e9galement fonctionner comme un symbole de la [r\u00e9publique hollandaise].",
  "input": "like the equally life size bird in jan asselijn 's the threatened swan the bull can also function as a symbol of the [dutch republic] ."
 },

I think I'm just going to skip duplicate examples. I think that's the best way to handle this.
It will trigger a few assertions for now, but I can just remove those.

The other issue is the period at the end. That should also be able to be fixed.

en-fr:
    Skipped 6086/16767 sentences
    Skipped 72 sentences because they were not found in the London results - this is due to the stable London translation being unable to bracket the entities in the sentence. (london_translate should have displayed an InvalidBracketing warning for these sentence.)
    Skipped 0 sentences because the number of entities in the sentence does not match the number of translated sentences. This should never happpen and is a sign that something is very wrong with matching the London examples to the MulDA examples. If this is more than 0, something is very wrong. Most likely, you have two examples with the exact same text but different IDs.
    Skipped 5984 sentences because the entity in the sentence could not be found in the translated sentence.
    Skipped 19 sentences because the entities in the sentence could not be unbracketed without confusion. This is due to the translation done in london_translate not being clear.

fr-en:
    Duplicate sentence id: e10abc93-6256-43a0-b0ac-d935dc94dd45
    Duplicate sentence id: dd775963-340f-4a69-aa8d-5037d3624a2e
    Duplicate sentence id: 0b581054-c451-41b9-bf7f-4857e4ff74c7
    Duplicate sentence id: b2f6f9ca-6d14-42bc-a117-6e8b153bcfc4
    Duplicate sentence id: 29041eca-f7a9-4ec3-a60f-e6c1ea1d4097
    Duplicate sentence id: 27e8edf1-3f22-49ef-bcaa-079ce1304f9c
    Duplicate sentence id: d541a48c-646f-46ac-8e48-a88e324147a3
    Duplicate sentence id: 0edbca16-ac11-46af-a042-3041d62d6291
    Duplicate sentence id: 2346a1ac-df75-46f6-9092-f6ca2f4322cc
    Duplicate sentence id: fc31d647-f1c2-4587-9f8d-528e4b867a00
    Duplicate sentence id: 1870982f-51c8-4722-8e41-f0d5f07fbaa3
    Duplicate sentence id: 059562b5-2bc4-4058-ad6b-9898934bb161
    Duplicate sentence id: 21379213-006d-4723-aa44-92f334b52c1f
    Duplicate sentence id: d5a34638-3bfe-421e-a9f9-a90941b4f9b8
    Duplicate sentence id: 8cbeb5ba-1cdb-41ff-a5d3-6db3522cf1ef
    Duplicate sentence id: 2a1090ac-3ea4-41ee-b560-a0fc14e6395a
    Duplicate sentence id: 5e2aee93-a9b0-4f21-afbd-c8c8410d41dc
    Duplicate sentence id: 77e81bf8-0a2b-41ab-9b9e-23c051bde708
    Duplicate sentence id: 3db2fe38-abdb-4077-8457-a041b637a058
    Duplicate sentence id: 6e63361e-b7bb-4534-bab5-cbac088bfa60
    Duplicate sentence id: 591da156-d990-4634-b7b7-6ace69cf5142
    Duplicate sentence id: dc6b7e04-c201-4c2c-b868-50984c03fa3e
    Duplicate sentence id: f3cbdb5b-2b6f-4c49-848d-e793d8257c93
    Duplicate sentence id: f9e30990-bdde-44ea-9768-05a834f88ac8
    Duplicate sentence id: 75627163-90bd-46d1-b7a6-e70c64dd2e2c
    Duplicate sentence id: ba49f431-c785-4416-8a9e-6ce9a18525ea
    Duplicate sentence id: 675709f9-d577-4dbc-8da6-5a0d550d028e
    Duplicate sentence id: c38f698c-37c5-40fb-a64d-47dbcbc29596

Great, another assertion failed.

assert len(entities) < len(london_results_dict[string_sentence]), f"Sentence entities: {entities}, London results: {london_results_dict[string_sentence]}"
AssertionError: Sentence entities: ['CarManufacturer0', 'ORG1'], London results: ['for [ford cosworth] as an engine builder.']

That assertion actually should have failed. This dataset is wild.
Here's the examples that caused that assertion to fail:
    # id c31e84aa-35f9-49d4-a250-4bc054ec6cde	domain=fr
    pour _ _ O
    ford _ _ B-CarManufacturer
    cosworth _ _ B-ORG
    en _ _ O
    tant _ _ O
    que _ _ O
    motoriste _ _ O
    . _ _ O

    # id 24bd2421-099b-4538-b920-bb506da5f469	domain=fr
    pour _ _ O
    ford _ _ B-ORG
    cosworth _ _ I-ORG
    en _ _ O
    tant _ _ O
    que _ _ O
    motoriste _ _ O
    . _ _ O


'''